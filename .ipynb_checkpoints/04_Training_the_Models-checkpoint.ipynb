{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, in many situations you don't really need to know the implementation details. However, having a good understanding of how thins work can help you quickly home in on the appropriate model, the right training alogrithm to use, and a good set of hyperparameters for your task.\n",
    "\n",
    "In this chapter, we will start by looking at the Linear Regression model, one of the simplest models there is. We will discuss 2 different ways to train it:\n",
    "- Using a direct \"closed-form\" equation that directly computes the model parameters that best fit the model to the training set(i.e., the model parameters that minimize the cost function over the training set).\n",
    "- Using an iterative optimization approach, called Gradient Descent(GD), that gradually tweaks the model parameters to minimize the cost function over the training set, eventually converging to the same set of parameters as the first method.\n",
    "\n",
    "Next will look at the Polynomial Regression, a more complex model that can fit nonliear datasets. It has more parameters than Linear Regression and is more prone to overfitting the training data. Then we will look at several regularization techniques that can reduce the risk of overfitting the training set.\n",
    "\n",
    "Finally, we will look at 2 more models that are commonly used for classification tasks:Logistic Regression and Softmax Regression.\n",
    "\n",
    "# Linear Regression\n",
    "Generally, a linear model makes a prediciton by simply computing a weighted sum of the input features, plus a constant called the *bias term or intercept term*.\n",
    "$$\\hat{y} = \\theta_0+\\theta_1x_1+\\theta_2x_2+\\dots++\\theta_nx_n$$\n",
    "- $\\hat{y}$ is the predicted value.\n",
    "- $n$ is the number of features.\n",
    "- $x_i$ is the i^th feature value.\n",
    "- $\\theta_j$ is the j^th model parameter(including the bias term \\theta_0 and the feature weights $\\theta_0,\\theta_1,\\theta_2,\\dots,\\theta_n$\n",
    "\n",
    "This can also be written much more concisely using a vectorized form.\n",
    "$$\\hat{y}=h_\\theta(X)=\\theta^T\\cdot X$$\n",
    "- $\\theta$ is the model's *parameter vector*, containing the bias term $\\theta_0$ and the feature weights $\\theta_0$ to $\\theta_n$\n",
    "- $\\theta^T$ is the transpose of $\\theta$( a row vector instead of a column vector.\n",
    "- $X$ is the instance's *feature vector*, containing $x_0$ to $x_n$, with $x_0$ is always to 1.\n",
    "- $\\theta^T\\cdot$ $X$ is the dot product of $\\theta^T$ and $X$\n",
    "- $h_\\theta$ is the hypothesis function, using the model parameters $\\theta$.\n",
    "\n",
    "So this is the LR model, now how do we train it? For this purpose, we first need a measure of how well(or poorly) the model fits the training data. A common performance measure of a regression model is the Root Mean Square Error(RMSE). Therefore, to train a LR model, you need to find the value of $\\theta$ that minimizes the RMSE. In practice, it is simpler to minimize the MSE than the RMSE, and it leads to the same results(because the value that minimizes a function also minimizes its square root).\n",
    "\n",
    "The MSE of a LR hypothesis $h_\\theta$ on a training set $X$ is calculated using this equation.$$MSE(X,h_\\theta)=\\frac{1}{m}\\sum_{i=1}^m(\\theta^T\\cdot X^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "NOTE: We write $h_\\theta$ instead of $h$ in order to make it clear that the model is parametrized by the vector $\\theta$. To simplify the notations, we will just write MSE($\\theta$) instead of $MSE(X,h_\\theta)$.\n",
    "\n",
    "## The Normal Equation\n",
    "To find the value of $\\theta$ that minimizes the cost function, there is a *closed-form solution*. In other words, a mathematical equation that gives the result directly. This is called the **Normal Equation**.$$\\hat{\\theta}=(X^T\\cdot X)^-1\\cdot X^T\\cdot y$$\n",
    "- $\\hat{\\theta}$ is the value of $\\theta$ that minimizes the cost function.\n",
    "- $y$ is the vector of target values containing $y^{(1)}$ to $y^{(m)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
