{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, in many situations you don't really need to know the implementation details. However, having a good understanding of how thins work can help you quickly home in on the appropriate model, the right training alogrithm to use, and a good set of hyperparameters for your task.\n",
    "\n",
    "In this chapter, we will start by looking at the Linear Regression model, one of the simplest models there is. We will discuss 2 different ways to train it:\n",
    "- Using a direct \"closed-form\" equation that directly computes the model parameters that best fit the model to the training set(i.e., the model parameters that minimize the cost function over the training set).\n",
    "- Using an iterative optimization approach, called Gradient Descent(GD), that gradually tweaks the model parameters to minimize the cost function over the training set, eventually converging to the same set of parameters as the first method.\n",
    "\n",
    "Next will look at the Polynomial Regression, a more complex model that can fit nonliear datasets. It has more parameters than Linear Regression and is more prone to overfitting the training data. Then we will look at several regularization techniques that can reduce the risk of overfitting the training set.\n",
    "\n",
    "Finally, we will look at 2 more models that are commonly used for classification tasks:Logistic Regression and Softmax Regression.\n",
    "\n",
    "# Linear Regression\n",
    "Generally, a linear model makes a prediciton by simply computing a weighted sum of the input features, plus a constant called the *bias term or intercept term*.\n",
    "$$\\hat{y} = \\theta_0+\\theta_1x_1+\\theta_2x_2+\\dots++\\theta_nx_n$$\n",
    "- $\\hat{y}$ is the predicted value.\n",
    "- $n$ is the number of features.\n",
    "- $x_i$ is the i^th feature value.\n",
    "- $\\theta_j$ is the j^th model parameter(including the bias term \\theta_0 and the feature weights $\\theta_0,\\theta_1,\\theta_2,\\dots,\\theta_n$\n",
    "\n",
    "This can also be written much more concisely using a vectorized form.\n",
    "$$\\hat{y}=h_\\theta(X)=\\theta^T\\cdot X$$\n",
    "- $\\theta$ is the model's *parameter vector*, containing the bias term $\\theta_0$ and the feature weights $\\theta_0$ to $\\theta_n$\n",
    "- $\\theta^T$ is the transpose of $\\theta$( a row vector instead of a column vector.\n",
    "- $X$ is the instance's *feature vector*, containing $x_0$ to $x_n$, with $x_0$ is always to 1.\n",
    "- $\\theta^T\\cdot$ $X$ is the dot product of $\\theta^T$ and $X$\n",
    "- $h_\\theta$ is the hypothesis function, using the model parameters $\\theta$.\n",
    "\n",
    "So this is the LR model, now how do we train it? For this purpose, we first need a measure of how well(or poorly) the model fits the training data. A common performance measure of a regression model is the Root Mean Square Error(RMSE). Therefore, to train a LR model, you need to find the value of $\\theta$ that minimizes the RMSE. In practice, it is simpler to minimize the MSE than the RMSE, and it leads to the same results(because the value that minimizes a function also minimizes its square root).\n",
    "\n",
    "The MSE of a LR hypothesis $h_\\theta$ on a training set $X$ is calculated using this equation.$$MSE(X,h_\\theta)=\\frac{1}{m}\\sum_{i=1}^m(\\theta^T\\cdot X^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "NOTE: We write $h_\\theta$ instead of $h$ in order to make it clear that the model is parametrized by the vector $\\theta$. To simplify the notations, we will just write MSE($\\theta$) instead of $MSE(X,h_\\theta)$.\n",
    "\n",
    "## The Normal Equation\n",
    "To find the value of $\\theta$ that minimizes the cost function, there is a *closed-form solution*. In other words, a mathematical equation that gives the result directly. This is called the **Normal Equation**.$$\\hat{\\theta}=(X^T\\cdot X)^{-1}\\cdot X^T\\cdot y$$\n",
    "- $\\hat{\\theta}$ is the value of $\\theta$ that minimizes the cost function.\n",
    "- $y$ is the vector of target values containing $y^{(1)}$ to $y^{(m)}$.\n",
    "\n",
    "Let's generate some linear-looking data to test this equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x183cc900358>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x183c3a49d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "X = 2*np.random.rand(100,1)\n",
    "y = 4+3*X + np.random.randn(100, 1)\n",
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute $\\hat{\\theta}$ using the Normal Equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.87708817],\n",
       "       [3.09517191]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b = np.c_[np.ones((100,1)), X] # add x0=1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would have hoped for $\\theta_0=4$ and $\\theta_1=3$ instead of $\\theta_0=3.762$ and $\\theta_1=3.129$. Closed enough, but the noise made it impossible to recover the exact parameters of the original function.\n",
    "\n",
    "Now we could use $\\hat{\\theta}$ to make predicitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.87708817],\n",
       "       [10.067432  ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0],[2]])\n",
    "X_new_b = np.c_[np.ones((2,1)), X_new] # add x0=1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x183c3a52400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_new, y_predict, 'r-')\n",
    "plt.plot(X,y,'b.')\n",
    "plt.axis([0,1,0,15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equivalent code using Scikit-Learn looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.87708817]), array([[3.09517191]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X,y)\n",
    "# 以下两个参数：截距，系数(1,2...n)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.87708817],\n",
       "       [10.067432  ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Complexity\n",
    "The Normal Equation gets very slow when the number of features grows large(e.g.,1000,000).\n",
    "\n",
    "So we will look at very different ways to train a Linear Model, **better suited for cases where there are a large number of features, or too many training instances to fit in memory.**\n",
    "\n",
    "## Gradient Descent\n",
    "**Gradient Descent** is a very genetic optimization algorithm capable of finding optimal solutions to a wide range of problems. **The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost funciton.**\n",
    "\n",
    "This is what GD exactly does: It measures the local gradient of the error function with regards to the parameter vector $\\theta$, and it goes in the direction of descending gradient. Once the gradient is zero, you have reached a minimum!\n",
    "\n",
    "Concretely, you start by filling $\\theta$ with random values(called **random initialization**), and then you improve it gradually, taking one baby step at a time, each step attempting to decrease the cost function(e.g., the MSE), until the algorithm **converges** to a minimum.\n",
    "\n",
    "An important parameter in Gradient Desent is the size of the steps, determined by the **learning rate** hyperparameter. If the learning rate is too small, then the algorithm wil have to go through many iterations to converge, which will take a long time. On the other hand, if the learning rate it too high, you might jump across the valley and endup on the other side, possibly enen higher up than you were before. This migh make the algorithm diverge(发散),  failing to find a good solution.\n",
    "\n",
    "### Two main challenges with Gradient Descent\n",
    "1. if the random initialization starts the algorithm on the left, then it will converge to a **local minimum**, which is not as good as the **global minimum**. \n",
    "2. If it starts on the right, then it will take a very long time to cross the plateau, and if you stop too early you will never reach the global minimum.\n",
    "\n",
    "**Fortunately, the MSE cost function for a Linear Regression model happens to be a *convex function*, which means that there is no local minimum, just one global minimum. It is also a continuous fuction with a slope that never changes abruptly.** These two facts have a great consequence: Gradient Descent is guaranteed to approach arbitrarily close the gloabal minimum(if wait long enough and the learning rate is not too high).\n",
    "\n",
    "**TIPS: When using Gradient Descent algorithm, ensure that all features have a similar scale**.\n",
    "\n",
    "Training a model means searching for a combination of model parameters that minimizes a cost function(over the training set). The more parameters a model has, the more dimensions the model's **parameter space** is, and the harder the search is.\n",
    "\n",
    "### Batch Gradient Descent\n",
    "To implement Gradient Descent, you need to calculate how much the cost function will change if you change $\\theta_j$ a little bit. This is called **partial derivative**.\n",
    "\n",
    "Equation4-5:partial derivatives of the cost functions:\n",
    "$$\\frac{\\partial}{\\partial\\theta_j}MSE(\\theta)=\\frac{2}{m}\\sum_{i=1}^m(\\theta^T\\cdot x^{(i)}-y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "Compute these gradients all in one go. THe gradient vector, noted $\\nabla_\\theta MSE(\\theta)$, contains all the partial derivatives of teh cost fuction(one for each model parameter).\n",
    "\n",
    "Equation4-6:partial derivatives of the cost functions:\n",
    "$$\\nabla_\\theta MSE(\\theta)=\\left(\n",
    "    \\begin{matrix}\n",
    "        \\frac{\\partial}{\\partial\\theta_0}MSE(\\theta)\\\\\n",
    "        \\frac{\\partial}{\\partial\\theta_1}MSE(\\theta)\\\\\n",
    "        \\vdots\\\\\n",
    "        \\frac{\\partial}{\\partial\\theta_n}MSE(\\theta)\\\\\n",
    "    \\end{matrix}\n",
    "\\right)\n",
    "= \\frac{2}{m}X^T\\cdot (X\\cdot \\theta - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
