{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines is very powerful, capable of performing linear or nonlinear classification, regressioin, and even outlier detection. SVMs are particularly well suited for classification of complex but small- or medium-sized datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM Classification\n",
    "You can think of an SVM classifier as fitting the widest possible street between the classes.This is called **large margin classification**.\n",
    "![fig 5-1](images/5-1.png)\n",
    "Notice that adding more training instances \"off the street\" will not affect the decision boundary at all:it is fully determined(or \"supported\") by the instances located on the edge of the street. These instances are called the support vectors.\n",
    "\n",
    "SVMs are sensitive to the feature scales, as you can see in fig 5-2.\n",
    "![fig 5-2](images/5-2.png)\n",
    "\n",
    "# Soft Margin Classification\n",
    "If we strictly impose that all instances be off the street and on the right side, this is called **hard margin classification**. There are 2 main issues with that.\n",
    "- It only works if the data is linearly separable.\n",
    "- It is quite sensitive to outliers.\n",
    "\n",
    "![fig 5-3](images/5-3.png)\n",
    "\n",
    "To avoid these issues it is preferable to use a more flexible model. The objective is to find a good balance between keeping the street as large as possible and limiting the margin violations. This is **Soft Margin Classification**.\n",
    "\n",
    "**In Scikit-Learn's SVM classes, you can control this balance using the C hyperparameter: a smaller C value leads to a wider street but more margin violations.** Fig 5-4 shows the decision boundaries and margins of two soft margin SVM classifiers on a nonlinearly separable dataset. On the left, using a high C value the classifier makes fewer margin violations but ends up with a smaller margin. On the right, using a low C value the margin is much larger, but many instances end up on the street. However, it seems likely that the second classifier will generalize better: in fact even on this training set it makes fewer prediction errors, since most of the margin violations are actually on the correct side of the decision boundary.\n",
    "![fig 5-4](images/5-4.png)\n",
    "\n",
    "**If your SVM model is overfitting, you can try regularizing it by reducing C**.\n",
    "\n",
    "Below are the codes of the right of fig 5-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linear_svc', LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris['data'][:,(2,3)] # petal length, petal width\n",
    "y = (iris['target']==2).astype(np.float64) # iris-virginica\n",
    "\n",
    "svm_clf = Pipeline((\n",
    "            ('scaler',StandardScaler()),\n",
    "            ('linear_svc',LinearSVC(C=1, loss='hinge')),\n",
    "        ))\n",
    "\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternatively, we could use the SVC class, using `SVC(kernel='linear',C=1)`, but it's much slower, especially with large training sets, so it is not recommended. Another option is to use the `SGDClassifier` class, with `SGDClassifier(loss='hinge', alpha=1/(m*C))`. This applies regular SGD to train a linear SVM classifier. It does not converge as fast as the LinearSVC class, but is can be useful to handle huge datasets taht do not fit in memory, or to handle online classification tasks.**\n",
    "\n",
    "**The LinearSVC class regularizes the bias term, so you should center the training set first by subtracting its means. This is automatic if you scale the data using the `StandardScaler`. Moreover, make sure you set the `loss` hyperparameter to `'hinge'`, as it is not the default value. Finally for better performance you should set the `dual` hyperparameter to `False`, unless there are more features than training instances.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear SVM Classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
