{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines is very powerful, capable of performing linear or nonlinear classification, regressioin, and even outlier detection. SVMs are particularly well suited for classification of complex but small- or medium-sized datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM Classification\n",
    "You can think of an SVM classifier as fitting the widest possible street between the classes.This is called **large margin classification**.\n",
    "![fig 5-1](images/5-1.png)\n",
    "Notice that adding more training instances \"off the street\" will not affect the decision boundary at all:it is fully determined(or \"supported\") by the instances located on the edge of the street. These instances are called the support vectors.\n",
    "\n",
    "SVMs are sensitive to the feature scales, as you can see in fig 5-2.\n",
    "![fig 5-2](images/5-2.png)\n",
    "\n",
    "# Soft Margin Classification\n",
    "If we strictly impose that all instances be off the street and on the right side, this is called **hard margin classification**. There are 2 main issues with that.\n",
    "- It only works if the data is linearly separable.\n",
    "- It is quite sensitive to outliers.\n",
    "\n",
    "![fig 5-3](images/5-3.png)\n",
    "\n",
    "To avoid these issues it is preferable to use a more flexible model. The objective is to find a good balance between keeping the street as large as possible and limiting the margin violations. This is **Soft Margin Classification**.\n",
    "\n",
    "**In Scikit-Learn's SVM classes, you can control this balance using the C hyperparameter: a smaller C value leads to a wider street but more margin violations.** Fig 5-4 shows the decision boundaries and margins of two soft margin SVM classifiers on a nonlinearly separable dataset. On the left, using a high C value the classifier makes fewer margin violations but ends up with a smaller margin. On the right, using a low C value the margin is much larger, but many instances end up on the street. However, it seems likely that the second classifier will generalize better: in fact even on this training set it makes fewer prediction errors, since most of the margin violations are actually on the correct side of the decision boundary.\n",
    "![fig 5-4](images/5-4.png)\n",
    "\n",
    "**If your SVM model is overfitting, you can try regularizing it by reducing C**.\n",
    "\n",
    "Below are the codes of the right of fig 5-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linear_svc', LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris['data'][:,(2,3)] # petal length, petal width\n",
    "y = (iris['target']==2).astype(np.float64) # iris-virginica\n",
    "\n",
    "svm_clf = Pipeline((\n",
    "            ('scaler',StandardScaler()),\n",
    "            ('linear_svc',LinearSVC(C=1, loss='hinge')),\n",
    "        ))\n",
    "\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternatively, we could use the SVC class, using `SVC(kernel='linear',C=1)`, but it's much slower, especially with large training sets, so it is not recommended. Another option is to use the `SGDClassifier` class, with `SGDClassifier(loss='hinge', alpha=1/(m*C))`. This applies regular SGD to train a linear SVM classifier. It does not converge as fast as the LinearSVC class, but is can be useful to handle huge datasets taht do not fit in memory, or to handle online classification tasks.**\n",
    "\n",
    "**The LinearSVC class regularizes the bias term, so you should center the training set first by subtracting its means. This is automatic if you scale the data using the `StandardScaler`. Moreover, make sure you set the `loss` hyperparameter to `'hinge'`, as it is not the default value. Finally for better performance you should set the `dual` hyperparameter to `False`, unless there are more features than training instances.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear SVM Classification\n",
    "One approach to handling nonlinear datasets is to add more features, such as polynomial features. Look at fig5-5.\n",
    "\n",
    "![fig 5-5](images/5-5.png)\n",
    "Here is an example. See result in Fig5-6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('poly_features', PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "polynomial_svm_clf = Pipeline((\n",
    "        ('poly_features', PolynomialFeatures(degree=3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svm_clf', LinearSVC(C=10, loss='hinge'))\n",
    "        ))\n",
    "polynomial_svm_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![fig 5-6](images/5-6.png)\n",
    "## Polynomial Kernel\n",
    "**Adding polynomial features is simple to implement and can work greate with all sorts of ML algorithms, but at a low polynomial degree it connot deal with very complex datasets, and with a high polynomial degree it creates a huge number of features, making the model too slow**.\n",
    "\n",
    "Fortunately, when using SVMs you can apply an almost miraculous(不可思议的) mathematical technique called the**Kernel Trick**. **It makes it possible to get the same result as if you added many polynomial features, without actually having to add them.**\n",
    "\n",
    "This trick is implemented by SVC class. Let's test it on the moons dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', SVC(C=5, cache_size=200, class_weight=None, coef0=1,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "poly_kernel_svm_clf = Pipeline((\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svm_clf', SVC(kernel='poly', degree=3,coef0=1, C=5))\n",
    "))\n",
    "poly_kernel_svm_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code trains an SVM classifier using a $3^{rd}-degree$ polynomial kernel. It is represented on the left of fig 5-7. On the right is using a $10^{th}-degree$ polynomial kernel. **Obviously, if your model is overfitting, reduce the polynomial degree. Conversely, if underfitting, try increasing it. The hyperparameter `coef0` controls how much the model is inflenced by high-degree polynomials versus low-degree polynomials. A common approach to find the right hyperparameter value is to use grid search.**\n",
    "\n",
    "![fig 5-7](images/5-7.png)\n",
    "\n",
    "## Adding Similarity Features\n",
    "**Another technique to tackle nonlinear problems is to add features computed using a similarity funciton that measures how much each instance resemaples a particular landmark.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
