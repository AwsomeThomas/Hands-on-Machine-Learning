{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Content\n",
    "- Train, visualize, and make predictions with Decision Trees.\n",
    "- CART training algorithm.\n",
    "- Regularize trees and use them for regression tasks.\n",
    "- Limitations of Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Visualizing a Decision Tree\n",
    "First build one and take a look at how Decision Trees make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:,2:] # petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the trained Decision Tree by using the `export_graphviz()` method to output a grph definition file called `iris_tree.dot`, then convert this dot file to a variety of formats such as PDF or PNG using the `dot` command-line tool from the `graphviz` package.\n",
    "`dot -Tpng iris_tree.dot -o iris_tree.png`. **First you need to install graphviz package and configure the system path.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "        tree_clf,\n",
    "        out_file = 'output/chapter5/iris_tree.dot',\n",
    "        feature_names=iris.feature_names[2:],\n",
    "        rounded=True,\n",
    "        filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![iris_tree](output/chapter5/iris_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions\n",
    "I think there is no need to explain the process of prediction. Here are the meanings of a node's attibutes.\n",
    "- Samples: the number of training samples it applies to.\n",
    "- value: number of instances of each class\n",
    "- gini: its impurity. If the instances in a node belong to the same class, it gini equals to 0.\n",
    "\n",
    "*Equation 6-1. Gini impurity*\n",
    "$$G_i=1-\\sum_{k=1}^np_{i,k}^2$$\n",
    "- $p_{i,k}$ is the ratio of class k instances among the training instances in the $i^{th}$ node.\n",
    "\n",
    "**One of the many qualities of Decision Trees is that they require very little data preparation. In particular, they don't require feature scaling or centering at all.**\n",
    "\n",
    "![fig 6-2](images/6-2.png)\n",
    "\n",
    "# Estimating Class Probabilities\n",
    "First it traverses the tree to find the leaf node for this instance, and then it returns the ratio of training instances of class k in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.90740741,  0.09259259]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5,1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5,1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CART Training Algorithm\n",
    "The idea is simple: the algorithm first splits the training set in two subsets using a single feature k and a threshold $t_k$. It searches for the pair $(k,t_k)$ that produces the purest subsets(weighted by their size). The cost function is Equation 6-2.\n",
    "\n",
    "*Equation 6-2. CART cost function for classification*\n",
    "$$J(k,t_k)=\\frac{m_{left}}{m}G_{left}+\\frac{m_{right}}{m}G_{right}$$\n",
    "- $G_{left/right}$ measures the impurity of the left/right subset.\n",
    "- $m_{left/right}$ is the number of instances in the left/right subset.\n",
    "\n",
    "It stops recursing once it reaches the maximum depth, or if it cannot find a split that will reduce impurity. So CART algorithm is a greedy algorithm. A greedy algorithm often produces a reasonably good solution, but it is not guaranteed to be the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gini Impurity or Entropy?\n",
    "**The concept of entropy originated in thermodynamics(热力学) as a measure of molecular disorder: entropy approaches zero when molecules are still and well ordered. It later spread to a wide variety of domains, including Shannon's information theory, where it measures the average information content of a message: entropy is zero when all messages are identical.** Equation 6-3 shows the difinition of the entropy of the $i^{th}$ node.\n",
    "\n",
    "*Equation 6-3. Entropy*\n",
    "$$H_i=-\\sum_{k=1}^{n} p_{i,k}log(p_{i,k})$$\n",
    "$${p_{i,k}}\\neq{0}$$\n",
    "\n",
    "Most of time these two lead to similar trees. Gini impurity is slightly faster to compute, so it is a good default. However, when they differ, Gini impurity tends to isolate the most frequent class in tis own branch of the tree, while entropy tends to produce slightly more balanced trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization Hyperparameters\n",
    "Decision Trees make very few assumptions about the training data. If unconstrained, the tree structure will adapt itself to the training data, fitting it very closely, and most likely overfitting it. Such a model is often called **nonparametric model** because the number of parameters is not determined prior to training. In contrast, a **parametric model** such as a linear model has a predetermined number of parameters, so its degree of freedom is limited, reducing the risk of overfitting.\n",
    "\n",
    "**In scikit-learn, recuding the `max_` or increasing the `min_` will regularize the model and thus reducing the risk of overfitting. Another way to avoid overfitting is pruning unnecessary nodes(剪枝法).**\n",
    "\n",
    "![fig 6-3](images/6-3.png)\n",
    "\n",
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig 6-4](images/6-4.png)\n",
    "\n",
    "![fig 6-5](images/6-5.png)\n",
    "\n",
    "The CART algorithm works mostly the same way as earlier. It now tries to split the training set in a way taht minimizes the MSE.\n",
    "\n",
    "![fig e6-4](images/e6-4.png)\n",
    "\n",
    "Just like for classification tasks, Decision Trees are prone to overfitting when dealing with regression tasks.\n",
    "\n",
    "![fig 6-6](images/6-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instability\n",
    "1. Decision Trees love orthogonal decision boundaries(**all splits are perpendicular to an axis**), which makes them sensitive to training set rotation. One way to limit this problem is to use PCA, which often results in a better orientation of the training data.\n",
    "\n",
    "![fig 6-7](images/6-7.png)\n",
    "\n",
    "2. The main issue with Decision Trees is that they are sensitive to small variations in the training data.**Pay attention to the one with petals 4.8 cm long and 1.8 cm wide**.\n",
    "\n",
    "![fig 6-2](images/6-2.png)\n",
    "\n",
    "![fig 6-8](images/6-8.png)\n",
    "\n",
    "**Random Forests can limit this instability by averagin predictioins over many trees.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
