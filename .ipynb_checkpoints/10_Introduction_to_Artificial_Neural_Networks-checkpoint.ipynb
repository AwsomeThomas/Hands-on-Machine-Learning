{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Content\n",
    "- ANN architectures\n",
    "- Multi-Layer Perceptrons\n",
    "- MNIST digit classification\n",
    "\n",
    "# From Biological to Artificial Neurons\n",
    "## Biological Neurons\n",
    "## Logical Computations with Neurons\n",
    "## The Perceptron\n",
    "It is based on a slightly different artificial neuron called a **linear threshold unit(LTU)**.\n",
    "\n",
    "![10](images/10-4.png)\n",
    "\n",
    "The most common step function used in Perceptron is the **Heaviside step function**. Sometimes **Sign function**.\n",
    "\n",
    "![10](images/e10-1.png)\n",
    "\n",
    "A perceptron is simply composed of a single layer of LTUs, with each neuron connected to all the inputs.\n",
    "\n",
    "![10](images/10-5.png)\n",
    "\n",
    "#### How is a perceptron trained?\n",
    "\n",
    "![10](images/e10-2.png)\n",
    "\n",
    "An example on the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\thomas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:,(2,3)] # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int) # Iris Setosa\n",
    "\n",
    "per_clf = Perceptron(random_state = 42)\n",
    "per_clf.fit(X,y)\n",
    "\n",
    "y_pred = per_clf.predict([[2,0.5]])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, Scikit learn's Perceptron class is equivalent to using an `SGDClassifier` with the hyperparameters:`loss='perceptron', learning_rate ='constant', eta0=1(learning rate), penalty=None(no regulazation)`\n",
    "\n",
    "**NOTE:** Perceptrons do not output a class probability as Logistic Regressioin does. They make predictions based on a hard threshold. So Logistic Regression is preferable.\n",
    "\n",
    "To solve trival problems like Exclusive OR(XOR) classification problem, many researchers dropped **connectionism** in favor of higher-level problems such as logic, problem solving and search. However, it turns out some of the limitations can be eliminated by stacking multiple Perceptrons, which is **Multi-layer Perceptron(MLP)**.\n",
    "\n",
    "![10](images/10-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron and Backpropagation\n",
    "\n",
    "![10](images/10-7.png)\n",
    "\n",
    "#### Backpropagation -- the first way to trian MLP. \n",
    "Today we would describe it as Gradient Descent using reverse-mode autodiff.\n",
    "\n",
    "**Description**: for each training instance the backpropagation algorithm first makes a prediction(forward pass), measures the error, then goes through each layer in reverse to measure the error contribution from each connection(reverse pass), and finally slightly tweaks the connection weights to reduce the error(Gradient Descent step).\n",
    "\n",
    "In order for this algorithm to work properly, the authors made a key change to MLP's architecture: **replaced the step function with the logistic function. $\\sigma(z)= 1/(1+exp(z))$**.\n",
    "\n",
    "This was essential because the step function contains only flat segments, so there is no gradient to work with, while the logistic function has a well-defined nonzero derivative everywhere, allowing GD to make some progress at every step.\n",
    "\n",
    "##### Other activation functions instead of Logistic function\n",
    "- The hyperbolik tangent funciton $tanh(z)=2\\sigma(2z)-1$:\n",
    "    - S shaped, continuous and differentiable\n",
    "    - output value ranges from -1 to 1, which tends to make each layer's output more or less normalized at the begining of training. THis helps speed up convergence.\n",
    "\n",
    "- The ReLU funciton $ReLU(z)=max(0,z)$:\n",
    "    - continuous\n",
    "    - not differentiable at z=0\n",
    "    - fast to compute\n",
    "    - does not have a maximum output value, which helps reduce some issues during Gradient Descent.\n",
    "    \n",
    "![10](images/10-8.png)\n",
    "\n",
    "![10](images/10-9.png)\n",
    "\n",
    "**Biological neurons seem to implement a roughly sigmoid (S-shaped) activation function. But it turns out that ReLU activation function generally works better in ANNs. This is one of the cases where the biological analogy was misleading.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an MLP with TensorFlow's High-Level API\n",
    "The `DNNClassifier` class makes it trivial to train a deep neural network with any number of hidden layers, and a softmax output layer to output estimated class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\thomas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-9f14a7b67b43>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\thomas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\thomas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting datasets/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\thomas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting datasets/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\thomas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting datasets/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\thomas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"datasets/MNIST_data/\", one_hot=True)\n",
    "\n",
    "# mnist = fetch_mldata('MNIST original')\n",
    "X_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I had some problems with the following codes, plz refer to this picture to understand.**\n",
    "```\n",
    "feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)\n",
    "# feature_columns = tf.contrib.estimator.multi_class_head(X_train)\n",
    "dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300,100],n_classes=10, feature_columns=feature_columns)\n",
    "dnn_clf.fit(x=X_train, y=y_train, batch_size=50, steps=40000)\n",
    "```\n",
    "\n",
    "![10](images/c10-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a DNN Using Plain TensorFlow\n",
    "**Take more control over the architecture of the network**.\n",
    "\n",
    "Implement Mini-batch Gradient Descent to train it on the MNIST data in two steps.\n",
    "## Constructioin Phase\n",
    "- First import libarary, then specify the number of inputs and outputs, and set the number of hidden neurons in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28*28 # size of an image\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 300\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Second, use placeholder nodes to represent the training data and targets.\n",
    "    - for the shape of X: the number of features is going to be 28*28 for one instance and the number of instances is unknown.\n",
    "    - y is 1D tensor with one entry per instance, but we don't know the size of the training batch at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.placeholder(tf.int64, shape=(None), name='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thirdly, create the actual neural network. \n",
    "    - The placeholder X will act as the input layer and it will be replaced with one training batch at a time(all the instances in a training batch will be processed simultaneously by the neural network).\n",
    "    - The two hidden layers are almost identical. The output layer is also similar, but it uses a softmax activation function instead of a ReLU activation function.\n",
    "\n",
    "So create a `neuron_layer()` function to create one layer at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2/np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name='weights')\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name='bias')\n",
    "        z = tf.matmul(X, W) + b\n",
    "        if activation=='relu':\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![10](images/tx10-1.png)\n",
    "\n",
    "#### Use neuron_layer function to create a neuron layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, \"hidden1\", activation='relu')\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, \"hidden2\", activation='relu')\n",
    "    logits = neuron_layer(hidden2, n_outputs, 'outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logits is the output of the neural network before going through the softmax activation function.\n",
    "\n",
    "#### Now do some optimizations\n",
    "Use the `fully_connected()` function to build a network instead of `neuron_layer()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = fully_connected(X, n_hidden1, scope='hidden1')\n",
    "    hidden2 = fully_connected(hidden1, n_hidden2, scope='hidden2')\n",
    "    logits = fully_connected(hidden2, n_outputs, scope='outputs', activation_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now define the cost function to train the neural network model.\n",
    "    - Cross entropy will penalize models that estimate a low probability for the target class. \n",
    "    - Use `sparse_soft_max_cross_entropy_with_logits()`: It computes the cross entropy based on the logits. It expects labels in the form of integers ranging from 0 to number of classes minus 1. (`soft_max_cross_entropy_with_logits()` takes labels in the form of one-hot vectors). This will give us a 1D tensor containing the cross entropy for each instance. \n",
    "    - Use `reduce_mean()` function to compute the mean cross entropy over all instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
