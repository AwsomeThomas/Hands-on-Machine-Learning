{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, in many situations you don't really need to know the implementation details. However, having a good understanding of how thins work can help you quickly home in on the appropriate model, the right training alogrithm to use, and a good set of hyperparameters for your task.\n",
    "\n",
    "In this chapter, we will start by looking at the Linear Regression model, one of the simplest models there is. We will discuss 2 different ways to train it:\n",
    "- Using a direct \"closed-form\" equation that directly computes the model parameters that best fit the model to the training set(i.e., the model parameters that minimize the cost function over the training set).\n",
    "- Using an iterative optimization approach, called Gradient Descent(GD), that gradually tweaks the model parameters to minimize the cost function over the training set, eventually converging to the same set of parameters as the first method.\n",
    "\n",
    "Next will look at the Polynomial Regression, a more complex model that can fit nonliear datasets. It has more parameters than Linear Regression and is more prone to overfitting the training data. Then we will look at several regularization techniques that can reduce the risk of overfitting the training set.\n",
    "\n",
    "Finally, we will look at 2 more models that are commonly used for classification tasks:Logistic Regression and Softmax Regression.\n",
    "\n",
    "# Linear Regression\n",
    "Generally, a linear model makes a prediciton by simply computing a weighted sum of the input features, plus a constant called the *bias term or intercept term*.\n",
    "$$\\hat{y} = \\theta_0+\\theta_1x_1+\\theta_2x_2+\\dots++\\theta_nx_n$$\n",
    "- $\\hat{y}$ is the predicted value.\n",
    "- $n$ is the number of features.\n",
    "- $x_i$ is the i^th feature value.\n",
    "- $\\theta_j$ is the j^th model parameter(including the bias term \\theta_0 and the feature weights $\\theta_0,\\theta_1,\\theta_2,\\dots,\\theta_n$\n",
    "\n",
    "This can also be written much more concisely using a vectorized form.\n",
    "$$\\hat{y}=h_\\theta(X)=\\theta^T\\cdot X$$\n",
    "- $\\theta$ is the model's *parameter vector*, containing the bias term $\\theta_0$ and the feature weights $\\theta_0$ to $\\theta_n$\n",
    "- $\\theta^T$ is the transpose of $\\theta$( a row vector instead of a column vector.\n",
    "- $X$ is the instance's *feature vector*, containing $x_0$ to $x_n$, with $x_0$ is always to 1.\n",
    "- $\\theta^T\\cdot$ $X$ is the dot product of $\\theta^T$ and $X$\n",
    "- $h_\\theta$ is the hypothesis function, using the model parameters $\\theta$.\n",
    "\n",
    "So this is the LR model, now how do we train it? For this purpose, we first need a measure of how well(or poorly) the model fits the training data. A common performance measure of a regression model is the Root Mean Square Error(RMSE). Therefore, to train a LR model, you need to find the value of $\\theta$ that minimizes the RMSE. In practice, it is simpler to minimize the MSE than the RMSE, and it leads to the same results(because the value that minimizes a function also minimizes its square root).\n",
    "\n",
    "The MSE of a LR hypothesis $h_\\theta$ on a training set $X$ is calculated using this equation.$$MSE(X,h_\\theta)=\\frac{1}{m}\\sum_{i=1}^m(\\theta^T\\cdot X^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "NOTE: We write $h_\\theta$ instead of $h$ in order to make it clear that the model is parametrized by the vector $\\theta$. To simplify the notations, we will just write MSE($\\theta$) instead of $MSE(X,h_\\theta)$.\n",
    "\n",
    "## The Normal Equation\n",
    "To find the value of $\\theta$ that minimizes the cost function, there is a *closed-form solution*. In other words, a mathematical equation that gives the result directly. This is called the **Normal Equation**.$$\\hat{\\theta}=(X^T\\cdot X)^{-1}\\cdot X^T\\cdot y$$\n",
    "- $\\hat{\\theta}$ is the value of $\\theta$ that minimizes the cost function.\n",
    "- $y$ is the vector of target values containing $y^{(1)}$ to $y^{(m)}$.\n",
    "\n",
    "Let's generate some linear-looking data to test this equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1ba7f5bf2b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmNCJiDzx/6t12XPWz81BAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ba756d3080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "X = 2*np.random.rand(100,1)\n",
    "y = 4+3*X + np.random.randn(100, 1)\n",
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute $\\hat{\\theta}$ using the Normal Equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.93207629],\n",
       "       [2.97168817]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b = np.c_[np.ones((100,1)), X] # add x0=1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would have hoped for $\\theta_0=4$ and $\\theta_1=3$ instead of $\\theta_0=3.762$ and $\\theta_1=3.129$. Closed enough, but the noise made it impossible to recover the exact parameters of the original function.\n",
    "\n",
    "Now we could use $\\hat{\\theta}$ to make predicitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.93207629],\n",
       "       [9.87545263]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0],[2]])\n",
    "X_new_b = np.c_[np.ones((2,1)), X_new] # add x0=1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ba76712fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_new, y_predict, 'r-')\n",
    "plt.plot(X,y,'b.')\n",
    "plt.axis([0,1,0,15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equivalent code using Scikit-Learn looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.93207629]), array([[2.97168817]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X,y)\n",
    "# 以下两个参数：截距，系数(1,2...n)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.93207629],\n",
       "       [9.87545263]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Complexity\n",
    "The Normal Equation gets very slow when the number of features grows large(e.g.,1000,000).\n",
    "\n",
    "So we will look at very different ways to train a Linear Model, **better suited for cases where there are a large number of features, or too many training instances to fit in memory.**\n",
    "\n",
    "## Gradient Descent\n",
    "**Gradient Descent** is a very genetic optimization algorithm capable of finding optimal solutions to a wide range of problems. **The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost funciton.**\n",
    "\n",
    "This is what GD exactly does: It measures the local gradient of the error function with regards to the parameter vector $\\theta$, and it goes in the direction of descending gradient. Once the gradient is zero, you have reached a minimum!\n",
    "\n",
    "Concretely, you start by filling $\\theta$ with random values(called **random initialization**), and then you improve it gradually, taking one baby step at a time, each step attempting to decrease the cost function(e.g., the MSE), until the algorithm **converges** to a minimum.\n",
    "\n",
    "An important parameter in Gradient Desent is the size of the steps, determined by the **learning rate** hyperparameter. If the learning rate is too small, then the algorithm wil have to go through many iterations to converge, which will take a long time. On the other hand, if the learning rate it too high, you might jump across the valley and endup on the other side, possibly enen higher up than you were before. This migh make the algorithm diverge(发散),  failing to find a good solution.\n",
    "\n",
    "### Two main challenges with Gradient Descent\n",
    "1. if the random initialization starts the algorithm on the left, then it will converge to a **local minimum**, which is not as good as the **global minimum**. \n",
    "2. If it starts on the right, then it will take a very long time to cross the plateau, and if you stop too early you will never reach the global minimum.\n",
    "\n",
    "**Fortunately, the MSE cost function for a Linear Regression model happens to be a *convex function*, which means that there is no local minimum, just one global minimum. It is also a continuous fuction with a slope that never changes abruptly.** These two facts have a great consequence: Gradient Descent is guaranteed to approach arbitrarily close the gloabal minimum(if wait long enough and the learning rate is not too high).\n",
    "\n",
    "**TIPS: When using Gradient Descent algorithm, ensure that all features have a similar scale**.\n",
    "\n",
    "Training a model means searching for a combination of model parameters that minimizes a cost function(over the training set). The more parameters a model has, the more dimensions the model's **parameter space** is, and the harder the search is.\n",
    "\n",
    "### Batch Gradient Descent\n",
    "To implement Gradient Descent, you need to calculate how much the cost function will change if you change $\\theta_j$ a little bit. This is called **partial derivative**.\n",
    "\n",
    "*Equation4-5:partial derivatives of the cost functions:*\n",
    "$$\\frac{\\partial}{\\partial\\theta_j}MSE(\\theta)=\\frac{2}{m}\\sum_{i=1}^m(\\theta^T\\cdot x^{(i)}-y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "Compute these gradients all in one go. THe gradient vector, noted $\\nabla_\\theta MSE(\\theta)$, contains all the partial derivatives of teh cost fuction(one for each model parameter).\n",
    "\n",
    "*Equation4-6:partial derivatives of the cost functions:*\n",
    "$$\\nabla_\\theta MSE(\\theta)=\\left(\n",
    "    \\begin{matrix}\n",
    "        \\frac{\\partial}{\\partial\\theta_0}MSE(\\theta)\\\\\n",
    "        \\frac{\\partial}{\\partial\\theta_1}MSE(\\theta)\\\\\n",
    "        \\vdots\\\\\n",
    "        \\frac{\\partial}{\\partial\\theta_n}MSE(\\theta)\\\\\n",
    "    \\end{matrix}\n",
    "\\right)\n",
    "= \\frac{2}{m}X^T\\cdot (X\\cdot \\theta - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this formula involves calculations over the full training set **X**, at each Gradient Descent step! This is why the algorithm is called *Batch Gradient Descent*: It uses the hole batch of training data at every step. As a result it is terribly slow on every large training sets. However, training a Linear Regression model when there are hundreds of thousands of features is much faster using Gradient Descent than using the Normal Equation.\n",
    "\n",
    "Once you have the gradient vector, which points uphill, just go in the opposite direction to go downhill. This means subtracting $\\nabla_\\theta MSE(\\theta)$ from $\\theta$. This is where the learning rate $\\eta$ comes to play: Multiply the gradient vector by $\\eta$ to determine the size of the downhill step.\n",
    "\n",
    "*Equation4-7: Gradient Descent step*\n",
    "$$\\theta^{next step}=\\theta-\\eta\\nabla_\\theta MSE(\\theta)$$\n",
    "Here is a quick implementation of this algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.93207629],\n",
       "       [2.97168817]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.1 #learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2,1) # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta*gradients\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.17382817e+75],\n",
       "       [-7.36694876e+75]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.5 #learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2,1) # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta*gradients\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.92009294],\n",
       "       [2.98173075]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.01 #learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2,1) # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta*gradients\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\eta = 0.1$, we get $\\theta$ exactly equal to what Normal Equation found. Gradient Descent worked perfectly. But when $\\eta =$ 0.5 or 0.01, the results changed. \n",
    "\n",
    "**How to find a good learning rate and iterations**:\n",
    "- To find a good learning rate, we can use grid search.\n",
    "- To  find a good iteration, a simple solution is to set a very large number of iterations but to interrupt the algorithm when the gradient vector becomes tiny -- that is, when its norm becomes smaller than a tiny number $\\epsilon$(called the **tolerance**) -- because this happens when Gradient Descent has (almost) reached the minimum.\n",
    "\n",
    "#### Convergence Rate\n",
    "When the cost function is convex and its slope does not change abruptly(e.g., MSE cost function), it can be shown that Batch Gradient Descent with a fixed learning rate has a **convergence rate** of $O(\\frac{1}{iterations})$. In other works, if you divide the tolerance $\\eta$ by 10, then the algorithm will have to run about 10 times more iterations.\n",
    "\n",
    "### Stochastic Gradient Descent(随机梯度下降)\n",
    "\n",
    "**The main problem with Batch Gradient Descent is the fact that it uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large.**\n",
    "\n",
    "**Stochastic Gradient Descent just picks A random instance in the training set at every step and computes the gradients based only on that single instance.** Obviously, this makes the algorithm much faster. SGD can be implemented as an out-of-core algorithm. \n",
    "\n",
    "On the other hand, due to its stochastic nature, this algorithm is much less regular than BGD: \n",
    "- instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. **Over time it will end up very close to the minimum, but not optimal.**\n",
    "- But this acutally help the algorithm jump out of local minima,**so SGD has a better chance of inding the global minimum than BGD does.**\n",
    "\n",
    "**One solution to this dilemma(进退两难) is to gradually reduce the learning rate.** The steps start out large, then get smaller and smaller, allowing the algorithm to settle at the global minimum. This process is called **simulated annealing(模拟退火)**. The function that determines the learning rate at each iteration is called the **learning schedule**.If the learning rate is reduced too quickly, you may get stuck in a local minimum, or even end up frozen halfway to the minimum. If the learning rate is reduced too slowly, you may jump around the minimum for a long tme and end up with a suboptimal solution if you halt training too early.\n",
    "\n",
    "A simple example of SGD using a simple leanring schedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 =5, 50 # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "theta = np.random.randn(2,1) #random initializaiton\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2*xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta*gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.93511201],\n",
       "       [3.00844469]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention we iterate by  rounds of m iterations; each round is called an **epoch**. While BGD code iterated 1000 times through the whole training set, this code goes through the training set only 50 times and reaches a fairly good solution.\n",
    "\n",
    "Note that sinece instances are picked randomly, some instances may be picked several times per epoch while others may not be picked at all. If you want to be sure that the algorithm goes through every instance at each epoch, another approach is to shuffle the training set, then go through it intance by instance, the shuffle it again, and so on. However, htis generally converges more slowly. \n",
    "\n",
    "An example of Linear Regression using SGD with Scikit-Learn is below. You can use `SGDRegressor` class, which defaults to optimizing the squared error cost function. The following code runs 50 epochs, starting with a learning rate of 0.1(`eta0=0.1`), using the default learning schedule(different from the preceding one), and it does not use any regulariztion(`penalty=None`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\thomas\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.1,\n",
       "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
       "       loss='squared_loss', max_iter=None, n_iter=50, penalty=None,\n",
       "       power_t=0.25, random_state=None, shuffle=True, tol=None, verbose=0,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(n_iter=50, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X,y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.96001876]), array([2.99725357]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch Gradient Descent\n",
    "**What is different in Mini-batch Gradient Descent:At each step, instead of computing the gradients based on the full training set(as in the Batch GD) or based on just on instance(as in Stochastic GD), Mini-batch GD computes the gradients on small random sets of instances called Mini-batches.** \n",
    "\n",
    "The main advantage of Mini-batch GD over Stochastic GD is that you can get a performance boost from hardwar optimization of matrix operation, especially when using GPUs. The algorithm's progress in parameter space is less erratic(飘忽不定的,因为SGD 是随机选取一个样本进行梯度计算） than with SGD, especially with fairly large mini-batches. As a result, Mini-batch GD will end up walking around a bit closer to the minimum than SGD. But, on the other hand, it may be harder for it to escape from local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
