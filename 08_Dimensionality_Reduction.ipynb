{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoushands or even millions of features not only make training extremely slow, also make it harder to find a good solution, this is called **the curse of dimensionality**.\n",
    "\n",
    "# Main content\n",
    "- the curse of dimensionality\n",
    "- know high-dimensional space\n",
    "- projection and Manifold learning\n",
    "- PCA, Kernel PCA, LLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Curse of Dimensionality\n",
    "The more dimensions the training set has, the greater the risk of overfitting it. In theory, one solution to the curse of dimensionality could be to increase the size of training set to reach a sufficient density of training instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Approaches for Dimensionality Reduction\n",
    "## Projection\n",
    "Training instances are not spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated. As a result, all training instances actually lie within a much lower-dimensional space. See an example.\n",
    "\n",
    "![8](images/8-2.png)\n",
    "\n",
    "![8](images/8-3.png)\n",
    "\n",
    "![8](images/8-4.png)\n",
    "\n",
    "## Manifold Learning\n",
    "\n",
    "![8](images/8-5.png)\n",
    "\n",
    "![8](images/8-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "Principal Component Analysis is the most popular dimensionality reduction algorithm. First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it.\n",
    "\n",
    "## Preserving the Variance\n",
    "Before projecting the training set onto a lower-dimensional htperplane, you first need to choose the right hyperplane.\n",
    "\n",
    "![8](images/8-7.png)\n",
    "\n",
    "## Principal Components\n",
    "**PCA identifies the axis that accounts for the largest amount of variance in the training set.**\n",
    "\n",
    "The unit vector that defines the $i^{th}$ axis is called the $i^{th}$ **principal component(PC)**. In fig 8-7, the 1st PC is $c_1$ and the 2nd PC is $c_2$.\n",
    "\n",
    "### How to find the PC of a training set?\n",
    "There is a standard matrix factorization technique called **Singular Value Decomposition(SVD)** that can decompose the training set matrix **X** into the ot product of three matrices $U\\cdot \\sum \\cdot V^T$, where $V^T$ contains all the principal components that we are looking for.\n",
    "\n",
    "![8](images/e8-1.png)\n",
    "\n",
    "Use Numpy's `svd()` function to obtain all the principal components of the training set, then extracts the first two PCs:\n",
    "```\n",
    "X_centered = X - X.mean(axis=0)\n",
    "U,s,V = np.linalg.svd(X_centered)\n",
    "c1 = V.T[:,0]\n",
    "c2 = V.T[:,1]\n",
    "```\n",
    "\n",
    "**PCA assumes that the dataset is centered around the origin. Don't forget to center the data first**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting Down to d Dimensions\n",
    "Once you have identified all the principal components, you can reduce the dimensionality of the dataset down to d dimensions by projecting it onto the hyperplane defined by the first d principal components. Selecting this hyperplane ensures that the projection will preserve as much variance as possible.\n",
    "\n",
    "$W_d$ is the matrix containing the first d principal components(i.e., the matrix composed of the first d columns of $V^T$.\n",
    "\n",
    "![8](images/e8-2.png)\n",
    "\n",
    "## Using Scikit-Learn\n",
    "```\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)\n",
    "\n",
    "first_PC = pca.components_.T[:,0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explained Variance Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
