{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoushands or even millions of features not only make training extremely slow, also make it harder to find a good solution, this is called **the curse of dimensionality**.\n",
    "\n",
    "# Main content\n",
    "- the curse of dimensionality\n",
    "- know high-dimensional space\n",
    "- projection and Manifold learning\n",
    "- PCA, Kernel PCA, LLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Curse of Dimensionality\n",
    "The more dimensions the training set has, the greater the risk of overfitting it. In theory, one solution to the curse of dimensionality could be to increase the size of training set to reach a sufficient density of training instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Approaches for Dimensionality Reduction\n",
    "## Projection\n",
    "Training instances are not spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated. As a result, all training instances actually lie within a much lower-dimensional space. See an example.\n",
    "\n",
    "![8](images/8-2.png)\n",
    "\n",
    "![8](images/8-3.png)\n",
    "\n",
    "![8](images/8-4.png)\n",
    "\n",
    "## Manifold Learning\n",
    "\n",
    "![8](images/8-5.png)\n",
    "\n",
    "![8](images/8-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "Principal Component Analysis is the most popular dimensionality reduction algorithm. First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it.\n",
    "\n",
    "## Preserving the Variance\n",
    "Before projecting the training set onto a lower-dimensional htperplane, you first need to choose the right hyperplane.\n",
    "\n",
    "![8](images/8-7.png)\n",
    "\n",
    "## Principal Components\n",
    "**PCA identifies the axis that accounts for the largest amount of variance in the training set.**\n",
    "\n",
    "The unit vector that defines the $i^{th}$ axis is called the $i^{th}$ **principal component(PC)**. In fig 8-7, the 1st PC is $c_1$ and the 2nd PC is $c_2$.\n",
    "\n",
    "### How to find the PC of a training set?\n",
    "There is a standard matrix factorization technique called **Singular Value Decomposition(SVD)** that can decompose the training set matrix **X** into the ot product of three matrices $U\\cdot \\sum \\cdot V^T$, where $V^T$ contains all the principal components that we are looking for.\n",
    "\n",
    "![8](images/e8-1.png)\n",
    "\n",
    "Use Numpy's `svd()` function to obtain all the principal components of the training set, then extracts the first two PCs:\n",
    "```\n",
    "X_centered = X - X.mean(axis=0)\n",
    "U,s,V = np.linalg.svd(X_centered)\n",
    "c1 = V.T[:,0]\n",
    "c2 = V.T[:,1]\n",
    "```\n",
    "\n",
    "**PCA assumes that the dataset is centered around the origin. Don't forget to center the data first**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting Down to d Dimensions\n",
    "Once you have identified all the principal components, you can reduce the dimensionality of the dataset down to d dimensions by projecting it onto the hyperplane defined by the first d principal components. Selecting this hyperplane ensures that the projection will preserve as much variance as possible.\n",
    "\n",
    "$W_d$ is the matrix containing the first d principal components(i.e., the matrix composed of the first d columns of $V^T$.\n",
    "\n",
    "![8](images/e8-2.png)\n",
    "\n",
    "## Using Scikit-Learn\n",
    "```\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)\n",
    "\n",
    "first_PC = pca.components_.T[:,0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explained Variance Ratio\n",
    "Another very useful piece of information is the **explained variance ratio** of each pricipal component. It indicates the proportion of the dataset's variance that lies along the axis of each principal component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Right Number of Dimensions\n",
    "Instead of arbitrarily choosing the number of dimensions to reduce down to, it is generally preferable to choose the number of dimensions that add up to a sufficiently large portion of the variance(e.g., 95%). \n",
    "\n",
    "```\n",
    "pca=PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >=0.95)+1\n",
    "```\n",
    "\n",
    "And a better option:\n",
    "```\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "```\n",
    "\n",
    "![8](images/8-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for Compression\n",
    "\n",
    "![8](images/8-9.png)\n",
    "\n",
    "![8](images/e8-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental PCA\n",
    "**One problem with the preceding implementation of PCA is that it requires the whole training set to fit in memory in order for the SVD algorithm to run**.\n",
    "\n",
    "**Incremental PCA(IPCA) algorithms:Split the training set into mini-batches and feed an IPCA one mini batch at a time.**\n",
    "\n",
    "- First implemention\n",
    "\n",
    "```\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_mnist, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "    \n",
    "X_mnist_reduced = inc_pca.transform(X_mnist)\n",
    "```\n",
    "\n",
    "\n",
    "- Second implemention. Numpy's `memmap` class, which allows you to manipulate a large array stored in a binary file on disk as if it were entirely in memory;the class loads only the data it needs in memory, when it needs it.\n",
    "\n",
    "```\n",
    "X_mm = np.memmap(filename, dtype='float32', mode='readonly', shape=(m,n))\n",
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)\n",
    "```\n",
    "\n",
    "\n",
    "## Randomized PCA\n",
    "It is a stochastic algorithm that quickly finds an approximation of the first d principal components. It is dramatically faster than the previous algorithms when d is much smaller than n.\n",
    "\n",
    "```\n",
    "rnd_pca = IncrementalPCA(n_components=154, svd_solver='randomized')\n",
    "X_reduced = rnd_pca.fit_transform(X_mnist)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCA(kPCA)\n",
    "It is often good at preserving clusters of instances after projection, or sometimes even unrolling datasets that lie close to a twisted manifold.\n",
    "\n",
    "```\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "rbf_pca = KernelPCA(n_components=2, kernel='rbf', gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "```\n",
    "\n",
    "![8](images/8-10.png)\n",
    "\n",
    "# Selecting a Kernel and Tuning Hyperparameters\n",
    "Dimensionality reduction is often a preparation step for a supervised learning task, so we can use grid search to select the kernel and hyperparameters that lead to the best performance on that task.\n",
    "\n",
    "For example, the following code creates a two-step pipeline, first reducing dimensionality to two dimensions using kPCA, then applying Logistic Regression for classification. Then it uses Grid SearchCV to find the best kernel and gamma value for kPCA in order to get the best classification accuracy at the end of the pipeline.\n",
    "\n",
    "![8](images/c8-2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
