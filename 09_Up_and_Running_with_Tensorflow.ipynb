{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Your First Graph and Running It in a Session\n",
    "\n",
    "![9](images/9-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.Variable(3, name='x')\n",
    "y = tf.Variable(4, name='y')\n",
    "f = x*x*y +y +2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important thing to understand is that this code does not actually perform any computation. It just creates a computation graph.\n",
    "\n",
    "To evaluate this graph, you need to open a TensorFlow session and use it to initialize the variables and evaluate f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)\n",
    "\n",
    "sess.close() # free up resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having to repeat `sess.run()` all the time is a bit cumbersome(麻烦的), but fortunately there is a better way:\n",
    "\n",
    "```\n",
    "with tf.Session() as sess:\n",
    "    x.initializer().run()\n",
    "    y.initializer().run()\n",
    "    result = f.eval()\n",
    "```\n",
    "\n",
    "**Error:'Operation' object is not callable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `with` block, the session is set as the default session.\n",
    "\n",
    "A TensorFlow program is typically split into two parts:\n",
    "1. **Construction phase**: builds a computation graph representing the ML model and the computations required to train it.\n",
    "2. **Execution phase**: runs a lood that evaluates a training setp repeatedly, gradually imporving the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Graphs\n",
    "Any node you create is automatically to the default graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases this is fine, but sometimes you may want to manage multiple independent graphs. You can do this by creating a new `Graph` and temporarily making it the default fraph inside a `with` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "    \n",
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In jupyter, it is common to run the same commands more than once. You may end up with a default graph containing many duplicate nodes. One solution is to restart the kernel while another solution is to just reset the default graph by running `tf.reset_default_graph()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lifecycle of a Node Value\n",
    "TensorFlow automatically determines the set of nodes that is depends on and it evaluates these nodes first. For example: \n",
    "- First, this code defines a very simple graph. \n",
    "- Then it starts a session and runs  the graph to evaluate y: TensorFlow automatically detects that y depends on w, which depends on x, so it first evaluates w, then x, then y, and return the value of y.\n",
    "- Finally, the code runs the graph to evaluate z.\n",
    "\n",
    "**It is important to note that it will not reuse the result of the previous evaluation of w and x. In short, the code evaluates w and x twice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w+2\n",
    "y = x+5\n",
    "z = x*3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval())\n",
    "    print(z.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All node values are dropped between graph runs, except variable values, which are maintained by the session across graph runs. A variable starts its life when its initializer is run, and it ends when the session is closed.**\n",
    "\n",
    "to Evaluate y and z efficiently, without evaluating w and x twice as in the previous code, ask TensorFlow to evaluate boty y and z in just one graph run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val,z_val = sess.run([y,z])\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In single-process TensorFlow, each session would have its own copy of every variable. In distributed TensorFlow, variable state is stored on the servers, not in the sessions, so multiple sessions can share the same varibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with TensorFlow\n",
    "TensorFlow operations(*ops* for short) can take any number of inputs and produce any number of outputs. **Constants and variables** take no input(they are called *source ops*). The inputs and outputs are multidimensional arrays, called *tensors*. In the python API tensors are simply represented by Numpy ndarrays. They typically contain floats, but you can also use them to carry strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.71851807e+01]\n",
      " [  4.36337471e-01]\n",
      " [  9.39523336e-03]\n",
      " [ -1.07113101e-01]\n",
      " [  6.44792199e-01]\n",
      " [ -4.03380000e-06]\n",
      " [ -3.78137082e-03]\n",
      " [ -4.23484027e-01]\n",
      " [ -4.37219113e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m,n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m,1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT,X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "    print(theta_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Gradient Descent\n",
    "**When using Gradient Descent, it is important to first bormalize the input feature vectors, or else trining may be much slower.**\n",
    "\n",
    "\n",
    "## Manually Computing the Gradient\n",
    "- `random_uniform()` function creates a node in the graph that will generate a tensor containing random values.\n",
    "- `assign()` function creates a node that will assign a new value to a variable.\n",
    "- The main loop executes the training step over and over again(`n_epochs` times) and every 100 iterations it prints out the current Mean Squared Error(mse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE= 7.2741\n",
      "Epoch 100 MSE= 1.69565\n",
      "Epoch 200 MSE= 1.32067\n",
      "Epoch 300 MSE= 1.24903\n",
      "Epoch 400 MSE= 1.19984\n",
      "Epoch 500 MSE= 1.15639\n",
      "Epoch 600 MSE= 1.11716\n",
      "Epoch 700 MSE= 1.0816\n",
      "Epoch 800 MSE= 1.04928\n",
      "Epoch 900 MSE= 1.01982\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "scaled_housing_data_plus_bias = min_max_scaler.fit_transform(housing_data_plus_bias)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0,1.0), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name= 'predictions')\n",
    "error = y_pred -y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta-learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 ==0:\n",
    "            print('Epoch', epoch, 'MSE=', mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using autodiff\n",
    "**TensorFlow's autodiff feature can automatically and efficiently compute the gradients, simply replace the `gradients = ...` line with `gradients=tf.gradients(mse, [theta])[0]`** \n",
    "\n",
    "The `gradients()` function takes an op(in this case mse) and a list of variables(in this case just theta), and it creates a list of ops(one per variable) to compute the gradients of the op with regards to each variable.\n",
    "\n",
    "![9](images/t9-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE= 9.36744\n",
      "Epoch 100 MSE= 1.91181\n",
      "Epoch 200 MSE= 1.42061\n",
      "Epoch 300 MSE= 1.33494\n",
      "Epoch 400 MSE= 1.27868\n",
      "Epoch 500 MSE= 1.22938\n",
      "Epoch 600 MSE= 1.185\n",
      "Epoch 700 MSE= 1.14483\n",
      "Epoch 800 MSE= 1.1083\n",
      "Epoch 900 MSE= 1.07499\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "scaled_housing_data_plus_bias = min_max_scaler.fit_transform(housing_data_plus_bias)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0,1.0), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name= 'predictions')\n",
    "error = y_pred -y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "#----------------------------\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "#----------------------------\n",
    "training_op = tf.assign(theta, theta-learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 ==0:\n",
    "            print('Epoch', epoch, 'MSE=', mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an Optimizer\n",
    "TensorFlow computes the gradients for us, and it could be easier. Replace the preceding `gradients = ..` and `training_op = ...` lines with the following code:\n",
    "\n",
    "```\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "```\n",
    "\n",
    "Momentum optimizer often converges much faster than GD.\n",
    "```\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE= 7.07728\n",
      "Epoch 100 MSE= 1.79089\n",
      "Epoch 200 MSE= 1.42029\n",
      "Epoch 300 MSE= 1.33912\n",
      "Epoch 400 MSE= 1.28144\n",
      "Epoch 500 MSE= 1.23088\n",
      "Epoch 600 MSE= 1.18562\n",
      "Epoch 700 MSE= 1.14483\n",
      "Epoch 800 MSE= 1.1079\n",
      "Epoch 900 MSE= 1.07432\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "scaled_housing_data_plus_bias = min_max_scaler.fit_transform(housing_data_plus_bias)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0,1.0), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name= 'predictions')\n",
    "error = y_pred -y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "#----------------------------\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "#----------------------------\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 ==0:\n",
    "            print('Epoch', epoch, 'MSE=', mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeding Data to the Training Algorithm\n",
    "To implement Mini-batch Gradient Descent, we need find a way to replace X and y at every iteration with the next mini-batch. The simplest way to do this is to use placeholder nodes. These nodes are special because they don't actually perform any computation, they just output the data you tell them to output at runtime. They are typically used to pass the training data to TensorFlow during training.\n",
    "\n",
    "To create a placeholder, you must call the `placeholder()` function and specify the output tensor's data type. Shape is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.  7.  8.]]\n",
      "[[  9.  10.  11.]\n",
      " [ 12.  13.  14.]]\n"
     ]
    }
   ],
   "source": [
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A+5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A:[[1,2,3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A:[[4,5,6],[7,8,9]]})\n",
    "    \n",
    "print(B_val_1)\n",
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![9](images/c9-1.png)\n",
    "\n",
    "# Saving and Restoring Models\n",
    "- Save trained models.\n",
    "- Save checkpoints at regular intervals during training so that if your computer crashes during training you can continue from the last checkpoint rather than start over from scratch.\n",
    "\n",
    "#### Two steps to save models:\n",
    "1. create a `Saver` node at the end of the construction phase(after all variable nodes are created)\n",
    "2. In the execution phase, call its `save()` method, passint ti the session and path of the checkpoint file.\n",
    "\n",
    "![9](images/c9-2.png)\n",
    "\n",
    "#### restoring a model\n",
    "1. create a `Saver` node at the end of the construction phase(after all variable nodes are created)\n",
    "2. At the begining of execution phase, call `restore()` method of `Saver()` object instead of initializing the variables.\n",
    "\n",
    "```\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, '/tmp/mymodel_final.ckpt')\n",
    "    [...]\n",
    "```\n",
    "\n",
    "By default a `Saver` saves and restores all variables under their own name, but we can specify which variables to save or restore, and what name to use.\n",
    "```\n",
    "saver = tf.train.Saver({'Weights':theta})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Graph and Training Curves Using TensorBoard\n",
    "- Step 1: write the graph definition and some training stats to a log directory that TensorBoard will read from. Use a different log directory every time you run your program, or else TensorBoard will merge stats from different runs, which will mess up the visualizations. The simplest solution for this is to include a timestamp in the log dirctory name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "root_logdir = 'tf_logs'\n",
    "log_dir = '{}/run-{}/'.format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 2: add the following code at the very end of the construction phase:\n",
    "    - first line: creates a node in the graph that will evaluate the MSE value and write it to a TensorBoard-compatible binary log string called a *summary*\n",
    "    - 2nd line: creates a `FileWriter` that you will use to write summaries to logfiles in the log girectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 3: update the execution phase to evaluate the `mse_summary` node regularly during training. This will output a summary that you can then write the events file using the `file_writer`.\n",
    "\n",
    "![9](images/c9-3.png)\n",
    "\n",
    "**Avoid logging training stats at every single training step, as this would significantly slow down training.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 4: close the `FileWriter` at the end of the program.\n",
    "```\n",
    "file_writer.close()\n",
    "```\n",
    "\n",
    "Use `tensorboard --logdir tf_logs/` to TensorBoard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Scopes\n",
    "Create `name scopes` to group related nodes.\n",
    "\n",
    "```\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularity\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
