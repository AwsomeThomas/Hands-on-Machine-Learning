{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Content\n",
    "- ANN architectures\n",
    "- Multi-Layer Perceptrons\n",
    "- MNIST digit classification\n",
    "\n",
    "# From Biological to Artificial Neurons\n",
    "## Biological Neurons\n",
    "## Logical Computations with Neurons\n",
    "## The Perceptron\n",
    "It is based on a slightly different artificial neuron called a **linear threshold unit(LTU)**.\n",
    "\n",
    "![10](images/10-4.png)\n",
    "\n",
    "The most common step function used in Perceptron is the **Heaviside step function**. Sometimes **Sign function**.\n",
    "\n",
    "![10](images/e10-1.png)\n",
    "\n",
    "A perceptron is simply composed of a single layer of LTUs, with each neuron connected to all the inputs.\n",
    "\n",
    "![10](images/10-5.png)\n",
    "\n",
    "#### How is a perceptron trained?\n",
    "\n",
    "![10](images/e10-2.png)\n",
    "\n",
    "An example on the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\thomas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:,(2,3)] # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int) # Iris Setosa\n",
    "\n",
    "per_clf = Perceptron(random_state = 42)\n",
    "per_clf.fit(X,y)\n",
    "\n",
    "y_pred = per_clf.predict([[2,0.5]])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, Scikit learn's Perceptron class is equivalent to using an `SGDClassifier` with the hyperparameters:`loss='perceptron', learning_rate ='constant', eta0=1(learning rate), penalty=None(no regulazation)`\n",
    "\n",
    "**NOTE:** Perceptrons do not output a class probability as Logistic Regressioin does. They make predictions based on a hard threshold. So Logistic Regression is preferable.\n",
    "\n",
    "To solve trival problems like Exclusive OR(XOR) classification problem, many researchers dropped **connectionism** in favor of higher-level problems such as logic, problem solving and search. However, it turns out some of the limitations can be eliminated by stacking multiple Perceptrons, which is **Multi-layer Perceptron(MLP)**.\n",
    "\n",
    "![10](images/10-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron and Backpropagation\n",
    "\n",
    "![10](images/10-7.png)\n",
    "\n",
    "#### Backpropagation -- the first way to trian MLP. \n",
    "Today we would describe it as Gradient Descent using reverse-mode autodiff.\n",
    "\n",
    "**Description**: for each training instance the backpropagation algorithm first makes a prediction(forward pass), measures the error, then goes through each layer in reverse to measure the error contribution from each connection(reverse pass), and finally slightly tweaks the connection weights to reduce the error(Gradient Descent step).\n",
    "\n",
    "In order for this algorithm to work properly, the authors made a key change to MLP's architecture: **replaced the step function with the logistic function. $\\sigma(z)= 1/(1+exp(z))$**.\n",
    "\n",
    "This was essential because the step function contains only flat segments, so there is no gradient to work with, while the logistic function has a well-defined nonzero derivative everywhere, allowing GD to make some progress at every step.\n",
    "\n",
    "##### Other activation functions instead of Logistic function\n",
    "- The hyperbolik tangent funciton $tanh(z)=2\\sigma(2z)-1$:\n",
    "    - S shaped, continuous and differentiable\n",
    "    - output value ranges from -1 to 1, which tends to make each layer's output more or less normalized at the begining of training. THis helps speed up convergence.\n",
    "\n",
    "- The ReLU funciton $ReLU(z)=max(0,z)$:\n",
    "    - continuous\n",
    "    - not differentiable at z=0\n",
    "    - fast to compute\n",
    "    - does not have a maximum output value, which helps reduce some issues during Gradient Descent.\n",
    "    \n",
    "![10](images/10-8.png)\n",
    "\n",
    "![10](images/10-9.png)\n",
    "\n",
    "**Biological neurons seem to implement a roughly sigmoid (S-shaped) activation function. But it turns out that ReLU activation function generally works better in ANNs. This is one of the cases where the biological analogy was misleading.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an MLP with TensorFlow's High-Level API\n",
    "The `DNNClassifier` class makes it trivial to train a deep neural network with any number of hidden layers, and a softmax output layer to output estimated class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"datasets/MNIST_data/\", one_hot=True)\n",
    "\n",
    "# mnist = fetch_mldata('MNIST original')\n",
    "X_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I had some problems with the following codes, plz refer to this picture to understand.**\n",
    "```\n",
    "feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)\n",
    "# feature_columns = tf.contrib.estimator.multi_class_head(X_train)\n",
    "dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300,100],n_classes=10, feature_columns=feature_columns)\n",
    "dnn_clf.fit(x=X_train, y=y_train, batch_size=50, steps=40000)\n",
    "```\n",
    "\n",
    "![10](images/c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
